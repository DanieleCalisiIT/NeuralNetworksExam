{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmHnXdXDqA8M"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install timm dill\n",
        "!pip install pytorch-ignite\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torchvision import utils as vutils\n",
        "from torchsummary import summary\n",
        "import ignite\n",
        "import math\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from ignite.engine import Engine, Events\n",
        "import ignite.distributed as idist\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu21JgQJqm63"
      },
      "outputs": [],
      "source": [
        "ignite.utils.manual_seed(999)\n",
        "ignite.utils.setup_logger(name=\"ignite.distributed.auto.auto_dataloader\", level=logging.WARNING)\n",
        "ignite.utils.setup_logger(name=\"ignite.distributed.launcher.Parallel\", level=logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZfQvu0urAhD"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/NN\n",
        "#!pip install --upgrade --no-cache-dir gdown\n",
        "#!gdown https://drive.google.com/u/0/uc?id=1aAJCZbXNHyraJ6Mi13dSbe7pTyfPXha0&export=download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokpMkG5cLFM"
      },
      "source": [
        "Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdun2oyjrmD4"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "image_size = 64\n",
        "batch_size = 16\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                        transforms.Resize((256, 256)),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_data = ImageFolder(\"train/cat-train\", transform=train_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICgLKZ1wtJZ2"
      },
      "outputs": [],
      "source": [
        "train_dataloader = idist.auto_dataloader(\n",
        "    train_data, \n",
        "    batch_size=batch_size, \n",
        "    num_workers=2, \n",
        "    shuffle=True, \n",
        "    drop_last=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNPtabQ1tYMR"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "real_batch = next(iter(train_dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzVccMiovlfD"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9g5Z5UyviOC"
      },
      "outputs": [],
      "source": [
        "def kaiming_init(module):\n",
        "    classname = module.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if \"Conv\" in classname:\n",
        "        try:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        except:\n",
        "            pass\n",
        "    elif \"BatchNorm\" in classname:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "def load_checkpoint(net, checkpoint):\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    temp = OrderedDict()\n",
        "    if 'state_dict' in checkpoint:\n",
        "        checkpoint = dict(checkpoint['state_dict'])\n",
        "    for k in checkpoint:\n",
        "        k2 = 'module.'+k if not k.startswith('module.') else k\n",
        "        temp[k2] = checkpoint[k]\n",
        "\n",
        "    net.load_state_dict(temp, strict=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_V1OixUvnkq"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ccB1Aavqb0"
      },
      "outputs": [],
      "source": [
        "class DiffAugment:\n",
        "    def __init__(self, policy='', channels_first=True):\n",
        "        self.policies = policy.split(',')\n",
        "        self.channels_first = channels_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.policies:\n",
        "            if not self.channels_first:\n",
        "                x = x.permute(0, 3, 1, 2)\n",
        "            for p in self.policies:\n",
        "                for f in AUGMENT_FNS[p]:\n",
        "                    x = f(x)\n",
        "            if not self.channels_first:\n",
        "                x = x.permute(0, 2, 3, 1)\n",
        "            x = x.contiguous()\n",
        "        return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct8qR1xLbPce"
      },
      "source": [
        "MODEL IMPLEMENTATION\n",
        "\n",
        "1.   DISCRIMINATOR\n",
        "2.   PROJECT GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_gd4vzjY3TP"
      },
      "source": [
        "DISCRIMINATOR - MULTI-SCALE DISCRIMINATOR (MSD)\n",
        "\n",
        "\n",
        "1.   Down-sampling Block for lower resolution inputs implementation\n",
        "2.   Multi Scale Discriminator implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVN_Hnn8aiDc"
      },
      "outputs": [],
      "source": [
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(c_in, c_out, 4, 2, 1)\n",
        "        self.bn = nn.BatchNorm2d(c_out)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return self.leaky_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c9qKzVPalVe"
      },
      "outputs": [],
      "source": [
        "class MultiScaleDiscriminator(nn.Module):\n",
        "    def __init__(self, channels, l):\n",
        "        super(MultiScaleDiscriminator, self).__init__()\n",
        "        self.head_conv = spectral_norm(nn.Conv2d(512, 1, 3, 1, 1))\n",
        "        layers = [DownBlock(channels, 64 * [1, 2, 4, 8][l - 1])] + [DownBlock(64 * i, 64 * i * 2) for i in [1, 2, 4][l - 1:]]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.optim = Adam(self.model.parameters(), lr=0.0002, betas=(0, 0.99))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return self.head_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDJxLLZOyl00"
      },
      "outputs": [],
      "source": [
        "class CSM(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation for the proposed Cross-Scale Mixing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, conv3_out_channels):\n",
        "        super(CSM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(channels, conv3_out_channels, 3, 1, 1)\n",
        "\n",
        "        for param in self.conv1.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.conv3.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.apply(kaiming_init)\n",
        "\n",
        "    def forward(self, high_res, low_res=None):\n",
        "        batch, channels, width, height = high_res.size()\n",
        "        if low_res is None:\n",
        "            # high_res_flatten = rearrange(high_res, \"b c h w -> b c (h w)\")\n",
        "            high_res_flatten = high_res.view(batch, channels, width * height)\n",
        "            high_res = self.conv1(high_res_flatten)\n",
        "            high_res = high_res.view(batch, channels, width, height)\n",
        "            high_res = self.conv3(high_res)\n",
        "            high_res = F.interpolate(high_res, scale_factor=2., mode=\"bilinear\")\n",
        "            return high_res\n",
        "        else:\n",
        "            high_res_flatten = high_res.view(batch, channels, width * height)\n",
        "            high_res = self.conv1(high_res_flatten)\n",
        "            high_res = high_res.view(batch, channels, width, height)\n",
        "            high_res = torch.add(high_res, low_res)\n",
        "            high_res = self.conv3(high_res)\n",
        "            high_res = F.interpolate(high_res, scale_factor=2., mode=\"bilinear\")\n",
        "            return high_res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLxy8cqPb6k2"
      },
      "source": [
        "EfficientNet-Lite Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXzT_YFQy7yb"
      },
      "outputs": [],
      "source": [
        "efficientnet_lite_params = {\n",
        "    # width_coefficient, depth_coefficient, image_size, dropout_rate\n",
        "    'efficientnet_lite0': [1.0, 1.0, 224, 0.2],\n",
        "    'efficientnet_lite1': [1.0, 1.1, 240, 0.2],\n",
        "    'efficientnet_lite2': [1.1, 1.2, 260, 0.3],\n",
        "    'efficientnet_lite3': [1.2, 1.4, 280, 0.3],\n",
        "    'efficientnet_lite4': [1.4, 1.8, 300, 0.3],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMb9fhK3zAcw"
      },
      "outputs": [],
      "source": [
        "def round_filters(filters, multiplier, divisor=8, min_width=None):\n",
        "    \"\"\"Calculate and round number of filters based on width multiplier.\"\"\"\n",
        "    if not multiplier:\n",
        "        return filters\n",
        "    filters *= multiplier\n",
        "    min_width = min_width or divisor\n",
        "    new_filters = max(min_width, int(filters + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_filters < 0.9 * filters:\n",
        "        new_filters += divisor\n",
        "    return int(new_filters)\n",
        "\n",
        "def round_repeats(repeats, multiplier):\n",
        "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
        "    if not multiplier:\n",
        "        return repeats\n",
        "    return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "def drop_connect(x, drop_connect_rate, training):\n",
        "    if not training:\n",
        "        return x\n",
        "    keep_prob = 1.0 - drop_connect_rate\n",
        "    batch_size = x.shape[0]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=x.dtype, device=x.device)\n",
        "    binary_mask = torch.floor(random_tensor)\n",
        "    x = (x / keep_prob) * binary_mask\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    def __init__(self, inp, final_oup, k, s, expand_ratio, se_ratio, has_se=False):\n",
        "        super(MBConvBlock, self).__init__()\n",
        "\n",
        "        self._momentum = 0.01\n",
        "        self._epsilon = 1e-3\n",
        "        self.input_filters = inp\n",
        "        self.output_filters = final_oup\n",
        "        self.stride = s\n",
        "        self.expand_ratio = expand_ratio\n",
        "        self.has_se = has_se\n",
        "        self.id_skip = True  # skip connection and drop connect\n",
        "\n",
        "        # Expansion phase\n",
        "        oup = inp * expand_ratio  # number of output channels\n",
        "        if expand_ratio != 1:\n",
        "            self._expand_conv = nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        self._depthwise_conv = nn.Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, padding=(k - 1) // 2, stride=s, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "        if self.has_se:\n",
        "            num_squeezed_channels = max(1, int(inp * se_ratio))\n",
        "            self._se_reduce = nn.Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
        "            self._se_expand = nn.Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
        "\n",
        "        # Output phase\n",
        "        self._project_conv = nn.Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._momentum, eps=self._epsilon)\n",
        "        self._relu = nn.ReLU6(inplace=True)\n",
        "\n",
        "    def forward(self, x, drop_connect_rate=None):\n",
        "        \"\"\"\n",
        "        :param x: input tensor\n",
        "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
        "        :return: output of block\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        identity = x\n",
        "        if self.expand_ratio != 1:\n",
        "            x = self._relu(self._bn0(self._expand_conv(x)))\n",
        "        x = self._relu(self._bn1(self._depthwise_conv(x)))\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_expand(self._relu(self._se_reduce(x_squeezed)))\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        if self.id_skip and self.stride == 1  and self.input_filters == self.output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, drop_connect_rate, training=self.training)\n",
        "            x += identity  # skip connection\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientNetLite(nn.Module):\n",
        "    def __init__(self, widthi_multiplier, depth_multiplier, num_classes, drop_connect_rate, dropout_rate):\n",
        "        super(EfficientNetLite, self).__init__()\n",
        "\n",
        "        # Batch norm parameters\n",
        "        momentum = 0.01\n",
        "        epsilon = 1e-3\n",
        "        self.drop_connect_rate = drop_connect_rate\n",
        "\n",
        "        mb_block_settings = [\n",
        "            #repeat|kernal_size|stride|expand|input|output|se_ratio\n",
        "            [1, 3, 1, 1, 32,  16,  0.25],\n",
        "            [2, 3, 2, 6, 16,  24,  0.25],\n",
        "            [2, 5, 2, 6, 24,  40,  0.25],\n",
        "            [3, 3, 2, 6, 40,  80,  0.25],\n",
        "            [3, 5, 1, 6, 80,  112, 0.25],\n",
        "            [4, 5, 2, 6, 112, 192, 0.25],\n",
        "            [1, 3, 1, 6, 192, 320, 0.25]\n",
        "        ]\n",
        "\n",
        "        # Stem\n",
        "        out_channels = 32\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Build blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for i, stage_setting in enumerate(mb_block_settings):\n",
        "            stage = nn.ModuleList([])\n",
        "            num_repeat, kernal_size, stride, expand_ratio, input_filters, output_filters, se_ratio = stage_setting\n",
        "            # Update block input and output filters based on width multiplier.\n",
        "            input_filters = input_filters if i == 0 else round_filters(input_filters, widthi_multiplier)\n",
        "            output_filters = round_filters(output_filters, widthi_multiplier)\n",
        "            num_repeat= num_repeat if i == 0 or i == len(mb_block_settings) - 1  else round_repeats(num_repeat, depth_multiplier)\n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "            if num_repeat > 1:\n",
        "                input_filters = output_filters\n",
        "                stride = 1\n",
        "            for _ in range(num_repeat - 1):\n",
        "                stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
        "\n",
        "            self.blocks.append(stage)\n",
        "\n",
        "        # Head\n",
        "        in_channels = round_filters(mb_block_settings[-1][5], widthi_multiplier)\n",
        "        out_channels = 1280\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.fc = torch.nn.Linear(out_channels, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        #print(x.shape)\n",
        "        x = self.stem(x)\n",
        "        idx = 0\n",
        "        for i, stage in enumerate(self.blocks):\n",
        "            for block in stage:\n",
        "                drop_connect_rate = self.drop_connect_rate\n",
        "                if drop_connect_rate:\n",
        "                    drop_connect_rate *= float(idx) / len(self.blocks)\n",
        "                x = block(x, drop_connect_rate)\n",
        "                idx +=1\n",
        "            if i in [1, 2, 3, 6]:\n",
        "                features.append(x)\n",
        "            #print(f\"After block{i}\", x.shape)\n",
        "        x = self.head(x)\n",
        "        #print(x.shape)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x, features\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 1.0/float(n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def load_pretrain(self, path):\n",
        "        state_dict = torch.load(path)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def build_efficientnet_lite(name, num_classes):\n",
        "    width_coefficient, depth_coefficient, _, dropout_rate = efficientnet_lite_params[name]\n",
        "    model = EfficientNetLite(width_coefficient, depth_coefficient, num_classes, 0.2, dropout_rate)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(net, checkpoint):\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    temp = OrderedDict()\n",
        "    if 'state_dict' in checkpoint:\n",
        "        checkpoint = dict(checkpoint['state_dict'])\n",
        "    for k in checkpoint:\n",
        "        k2 = 'module.'+k if not k.startswith('module.') else k\n",
        "        temp[k2] = checkpoint[k]\n",
        "\n",
        "    net.load_state_dict(temp, strict=True)\n",
        "\n",
        "model_name = 'efficientnet_lite1'\n",
        "model = build_efficientnet_lite(model_name, 1000)\n",
        "\n",
        "use_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "  use_gpu = True\n",
        "model.load_state_dict(torch.load(\"efficientnet_lite1.pth\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNE3tvUhckJE"
      },
      "source": [
        "FastGAN Generator Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0qGshRQ0Lc8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generator architecture and code taken from \"Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis\" (arxiv.org/abs/2101.04775) and github.com/odegeasslbc/FastGAN-pytorch, respectively.\n",
        "\"\"\"\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "def conv2d(*args, **kwargs):\n",
        "    return spectral_norm(nn.Conv2d(*args, **kwargs))\n",
        "\n",
        "\n",
        "def convTranspose2d(*args, **kwargs):\n",
        "    return spectral_norm(nn.ConvTranspose2d(*args, **kwargs))\n",
        "\n",
        "\n",
        "def batchNorm2d(*args, **kwargs):\n",
        "    return nn.BatchNorm2d(*args, **kwargs)\n",
        "\n",
        "\n",
        "def linear(*args, **kwargs):\n",
        "    return spectral_norm(nn.Linear(*args, **kwargs))\n",
        "\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        nc = x.size(1)\n",
        "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
        "        nc = int(nc / 2)\n",
        "        return x[:, :nc] * torch.sigmoid(x[:, nc:])\n",
        "\n",
        "\n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "    def forward(self, feat, noise=None):\n",
        "        if noise is None:\n",
        "            batch, _, height, width = feat.shape\n",
        "            noise = torch.randn(batch, 1, height, width).to(feat.device)\n",
        "\n",
        "        return feat + self.weight * noise\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, feat):\n",
        "        return feat * torch.sigmoid(feat)\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super().__init__()\n",
        "\n",
        "        self.main = nn.Sequential(nn.AdaptiveAvgPool2d(4),\n",
        "                                  conv2d(ch_in, ch_out, 4, 1, 0, bias=False), Swish(),\n",
        "                                  conv2d(ch_out, ch_out, 1, 1, 0, bias=False), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, feat_small, feat_big):\n",
        "        return feat_big * self.main(feat_small)\n",
        "\n",
        "\n",
        "class InitLayer(nn.Module):\n",
        "    def __init__(self, nz, channel):\n",
        "        super().__init__()\n",
        "        self.init = nn.Sequential(\n",
        "            convTranspose2d(nz, channel * 2, 4, 1, 0, bias=False),\n",
        "            batchNorm2d(channel * 2), GLU())\n",
        "\n",
        "    def forward(self, noise):\n",
        "        noise = noise.view(noise.shape[0], -1, 1, 1)\n",
        "        return self.init(noise)\n",
        "\n",
        "\n",
        "def UpBlock(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        conv2d(in_planes, out_planes * 2, 3, 1, 1, bias=False),\n",
        "        batchNorm2d(out_planes * 2), GLU())\n",
        "    return block\n",
        "\n",
        "\n",
        "def UpBlockComp(in_planes, out_planes):\n",
        "    block = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        conv2d(in_planes, out_planes * 2, 3, 1, 1, bias=False),\n",
        "        NoiseInjection(),\n",
        "        batchNorm2d(out_planes * 2), GLU(),\n",
        "        conv2d(out_planes, out_planes * 2, 3, 1, 1, bias=False),\n",
        "        NoiseInjection(),\n",
        "        batchNorm2d(out_planes * 2), GLU()\n",
        "    )\n",
        "    return block\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngf=64, nz=100, nc=3, im_size=1024):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        nfc_multi = {4: 16, 8: 8, 16: 4, 32: 2, 64: 2, 128: 1, 256: 0.5, 512: 0.25, 1024: 0.125}\n",
        "        nfc = {}\n",
        "        for k, v in nfc_multi.items():\n",
        "            nfc[k] = int(v * ngf)\n",
        "\n",
        "        self.im_size = im_size\n",
        "\n",
        "        self.init = InitLayer(nz, channel=nfc[4])\n",
        "\n",
        "        self.feat_8 = UpBlockComp(nfc[4], nfc[8])\n",
        "        self.feat_16 = UpBlock(nfc[8], nfc[16])\n",
        "        self.feat_32 = UpBlockComp(nfc[16], nfc[32])\n",
        "        self.feat_64 = UpBlock(nfc[32], nfc[64])\n",
        "        self.feat_128 = UpBlockComp(nfc[64], nfc[128])\n",
        "        self.feat_256 = UpBlock(nfc[128], nfc[256])\n",
        "\n",
        "        self.se_64 = SEBlock(nfc[4], nfc[64])\n",
        "        self.se_128 = SEBlock(nfc[8], nfc[128])\n",
        "        self.se_256 = SEBlock(nfc[16], nfc[256])\n",
        "\n",
        "        self.to_big = conv2d(nfc[im_size], nc, 3, 1, 1, bias=False)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        feat_4 = self.init(x)\n",
        "        feat_8 = self.feat_8(feat_4)\n",
        "        feat_16 = self.feat_16(feat_8)\n",
        "        feat_32 = self.feat_32(feat_16)\n",
        "\n",
        "        feat_64 = self.se_64(feat_4, self.feat_64(feat_32))\n",
        "\n",
        "        feat_128 = self.se_128(feat_8, self.feat_128(feat_64))\n",
        "\n",
        "        feat_256 = self.se_256(feat_16, self.feat_256(feat_128))\n",
        "\n",
        "        return self.to_big(feat_256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y3HBUDJrSjw"
      },
      "outputs": [],
      "source": [
        "image_size = 256\n",
        "loadCheckpoint = \"\"\n",
        "checkpoint_path = \"afhq-cat2/\"\n",
        "lr = 0.0002\n",
        "beta1 = 0.0\n",
        "beta2 = 0.999\n",
        "checkpoint_efficient_net = \"efficientnet_lite1.pth\"\n",
        "latent_dim = 100\n",
        "epochs = 50\n",
        "diff_aug = True\n",
        "batch_size = 16\n",
        "log_every = 100\n",
        "\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNpMHqKWMN6M"
      },
      "outputs": [],
      "source": [
        "def get_feature_channels():\n",
        "    sample = torch.randn(1, 3, img_size, img_size)\n",
        "    _, features = efficient_net(sample)\n",
        "    return [f.shape[1] for f in features]\n",
        "    \n",
        "def csm_forward(features):\n",
        "    features = features[::-1]\n",
        "    csm_features = []\n",
        "    for i, csm in enumerate(csms):\n",
        "        if i == 0:\n",
        "            d = csm(features[i])\n",
        "            csm_features.append(d)\n",
        "        else:\n",
        "            d = csm(features[i], d)\n",
        "            csm_features.append(d)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8UxsROWoRve"
      },
      "outputs": [],
      "source": [
        "img_size = image_size\n",
        "gen = idist.auto_model(Generator(im_size=image_size))\n",
        "\n",
        "summary(gen, (latent_dim, 1, 1))\n",
        "\n",
        "if loadCheckpoint:\n",
        "    gen.load_state_dict(torch.load(os.path.join(loadCheckpoint,\"Generator.pth\")))\n",
        "    gen.train()\n",
        "\n",
        "gen_optim = Adam(gen.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "efficient_net = build_efficientnet_lite(\"efficientnet_lite1\", 1000)\n",
        "efficient_net = nn.DataParallel(efficient_net)\n",
        "checkpoint = torch.load(checkpoint_efficient_net)\n",
        "load_checkpoint(efficient_net, checkpoint)\n",
        "print(\"carico il checkpoint di efficientnet\")\n",
        "\n",
        "efficient_net.eval()\n",
        "\n",
        "feature_sizes = get_feature_channels()\n",
        "\n",
        "#summary(gen,(latent_dim, 1, 1))\n",
        "\n",
        "\n",
        "csms = nn.ModuleList([\n",
        "    CSM(feature_sizes[3], feature_sizes[2]),\n",
        "    CSM(feature_sizes[2], feature_sizes[1]),\n",
        "    CSM(feature_sizes[1], feature_sizes[0]),\n",
        "    CSM(feature_sizes[0], feature_sizes[0]),\n",
        "])\n",
        "\n",
        "\n",
        "if loadCheckpoint:\n",
        "    for i in range(len(csms) - 1):\n",
        "        csms[i].load_state_dict(torch.load(os.path.join(loadCheckpoint,f\"CSM_{i}.pth\")))\n",
        "        csms[i].train()\n",
        "        \n",
        "\n",
        "discs = idist.auto_model(nn.ModuleList([\n",
        "    MultiScaleDiscriminator(feature_sizes[0], 1),\n",
        "    MultiScaleDiscriminator(feature_sizes[1], 2),\n",
        "    MultiScaleDiscriminator(feature_sizes[2], 3),\n",
        "    MultiScaleDiscriminator(feature_sizes[3], 4),\n",
        "][::-1]))\n",
        "\n",
        "disc = idist.auto_model(MultiScaleDiscriminator(feature_sizes[0], 1))\n",
        "\n",
        "if loadCheckpoint:\n",
        "    for i in range(len(discs) - 1):\n",
        "        discs[i].load_state_dict(torch.load(os.path.join(loadCheckpoint,f\"Discriminator_{i}.pth\")))\n",
        "        discs[i].train()\n",
        "\n",
        "if loadCheckpoint:\n",
        "    print(f\"Checkpoint at : {loadCheckpoint} successfully loaded\")\n",
        "\n",
        "augmentations = 'color,translation,cutout'\n",
        "\n",
        "DiffAug = DiffAugment(augmentations)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zzDe-unZdyEh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "logging.info(f\"Using device: {device}\")\n",
        "\n",
        "gen.to(device)\n",
        "efficient_net.to(device)\n",
        "\n",
        "for disc in discs:\n",
        "    disc.to(device)\n",
        "for csm in csms:\n",
        "    csm.to(device)\n",
        "\n",
        "disc_losses = []\n",
        "gen_losses = []\n",
        "\n",
        "ckpts_outputs_path = os.path.join(checkpoint_path, \"ckpts_outputs\")\n",
        "if not os.path.exists(ckpts_outputs_path):\n",
        "    os.mkdir(ckpts_outputs_path)\n",
        "\n",
        "if loadCheckpoint:\n",
        "    starting_epoch = int(os.path.basename(loadCheckpoint)) + 1\n",
        "    logging.info(f\"Resuming from epoch {starting_epoch}\")\n",
        "else:\n",
        "    starting_epoch = 0\n",
        "\n",
        "for epoch in range(starting_epoch, epochs + starting_epoch):\n",
        "    logging.info(f\"Starting epoch {epoch}\")\n",
        "\n",
        "\n",
        "\n",
        "    for i, (real_imgs, _) in enumerate(train_dataloader):\n",
        "        # pass the real batch to cuda\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        # geneate a random batch and pass it to cuda\n",
        "        z = torch.randn(real_imgs.shape[0], latent_dim)\n",
        "        z = z.to(device)\n",
        "        \n",
        "        gen_imgs_disc = gen(z).detach()\n",
        "        batch_gen_imgs = torch.clone(gen_imgs_disc)\n",
        "\n",
        "        # apply augmentation\n",
        "        if diff_aug:\n",
        "            gen_imgs_disc = DiffAug.forward(gen_imgs_disc)\n",
        "            real_imgs = DiffAug.forward(real_imgs)\n",
        "\n",
        "        # get efficient net features\n",
        "        _, features_fake = efficient_net(gen_imgs_disc)\n",
        "        _, features_real = efficient_net(real_imgs)\n",
        "\n",
        "        # feed efficient net features through CSM\n",
        "        features_real = csm_forward(features_real)\n",
        "        features_fake = csm_forward(features_fake)\n",
        "\n",
        "        # Train Discriminators:\n",
        "\n",
        "        for feature_real, feature_fake, disc in zip(features_real, features_fake, discs):\n",
        "            disc.optim.zero_grad()\n",
        "            y_hat_real = disc(feature_real)  # Cx4x4\n",
        "            y_hat_fake = disc(feature_fake)  # Cx4x4\n",
        "            y_hat_real = y_hat_real.sum(1)  # sum along channels axis\n",
        "            y_hat_fake = y_hat_fake.sum(1)\n",
        "            loss_real = torch.mean(F.relu(1. - y_hat_real))\n",
        "            loss_fake = torch.mean(F.relu(1. + y_hat_fake))\n",
        "            disc_loss = loss_real + loss_fake\n",
        "            disc_loss.backward(retain_graph=True)\n",
        "            disc.optim.step()\n",
        "            disc_losses.append(disc_loss.cpu().detach().numpy())\n",
        "\n",
        "        # Train Generator: z is the model of the generator\n",
        "        z = torch.randn(real_imgs.shape[0], latent_dim)\n",
        "        z = z.to(device)\n",
        "\n",
        "        gen_imgs_gen = gen(z)\n",
        "\n",
        "        if diff_aug:\n",
        "            gen_imgs_gen = DiffAug.forward(gen_imgs_gen)\n",
        "\n",
        "        # get efficient net features\n",
        "        _, features_fake = efficient_net(gen_imgs_gen)\n",
        "\n",
        "        # feed efficient net features through CSM\n",
        "        features_fake = csm_forward(features_fake)\n",
        "\n",
        "        gen_loss = 0.\n",
        "        gen_optim.zero_grad()\n",
        "\n",
        "        for feature_fake, disc in zip(features_fake, discs):\n",
        "            y_hat = disc(feature_fake)\n",
        "            y_hat = y_hat.sum(1)\n",
        "            gen_loss = -torch.mean(y_hat)\n",
        "        gen_loss.backward()\n",
        "        gen_optim.step()\n",
        "        gen_losses.append(gen_loss)\n",
        "\n",
        "        if i % log_every == 0:\n",
        "            path = os.path.join(checkpoint_path, str(epoch))\n",
        "            if not os.path.exists(path):\n",
        "                os.mkdir(path)\n",
        "            with torch.no_grad():\n",
        "                vutils.save_image(batch_gen_imgs.add(1).mul(0.5), os.path.join(ckpts_outputs_path, f'{epoch}_{i}.jpg'), nrow=4)\n",
        "            logging.info(f\"Iteration {i}: Gen Loss = {gen_loss}, Disc Loss = {disc_losses}.\")\n",
        "            torch.save(gen.state_dict(), os.path.join(path, \"Generator.pth\"))\n",
        "            for j in range(len(discs)):\n",
        "                torch.save(discs[j].state_dict(), os.path.join(path, f\"Discriminator_{j}.pth\"))\n",
        "                torch.save(csms[j].state_dict(), os.path.join(path, f\"CSM_{j}.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtmdT60dbp6N"
      },
      "source": [
        "Images Generator: starting from a weights path generates n_images with images_size=256x256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83TOtFk51ozG"
      },
      "outputs": [],
      "source": [
        "image_size=256\n",
        "weights_path=\"afhq-dog/44/Generator.pth\"\n",
        "verbose=True\n",
        "out_dir=\"./afhq-dog-generated\"\n",
        "n_images=100\n",
        "latent_dim=100\n",
        "mode=0\n",
        "grid_size=8\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Using device : \"+ device)\n",
        "\n",
        "# Loading state dict\n",
        "gen = Generator(im_size=image_size)\n",
        "gen.load_state_dict(torch.load(weights_path))\n",
        "gen = gen.to(device)\n",
        "if verbose:\n",
        "    print(\"Successfully loaded generator's state dict at : \" + weights_path)\n",
        "# Creating the output dir\n",
        "if not os.path.exists(out_dir):\n",
        "    if verbose:\n",
        "        print(\"Creating the output dir : \",out_dir)\n",
        "    os.mkdir(out_dir)\n",
        "\n",
        "#Initializing list for grid\n",
        "gen_imgs_list=[]\n",
        "fake_imgs=[]\n",
        "\n",
        "for i in tqdm(range(1,n_images + 1), position=0, leave=True):\n",
        "    \n",
        "    noise = torch.randn(latent_dim,1,1,device=device)\n",
        "    #Generating single image\n",
        "    gen_img = gen(noise.unsqueeze(0))\n",
        "    \n",
        "    #Saving the indivudual image\n",
        "    if mode in [0,2]:\n",
        "        img_path = os.path.join(out_dir,f\"img-{i}.jpg\")\n",
        "        save_image(gen_img, img_path)\n",
        "        fake_imgs.append(gen_img)\n",
        "        \n",
        "    \n",
        "    # Saving grid of images (N x N)\n",
        "    if mode in [1,2]:\n",
        "        gen_imgs_list.append(gen_img.detach().cpu().squeeze(0))\n",
        "        if len(gen_imgs_list) == grid_size ** 2:\n",
        "            grid_img_path=os.path.join(out_dir,f\"grid-imgs-{(i - grid_size ** 2 ) + 1}-{i}.jpg\")\n",
        "            save_image( gen_imgs_list, grid_img_path,nrow=grid_size)\n",
        "            gen_imgs_list = []\n",
        "\n",
        "# If there are remaining images in the grid array\n",
        "if len(gen_imgs_list) and mode in [1,2]:\n",
        "    save_image(gen_imgs_list ,os.path.join(out_dir,f\"grid-imgs-{i - len(gen_imgs_list)}-{i}-remaining_batch.jpg\"),nrow=grid_size)\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BJwHpn03QS9"
      },
      "outputs": [],
      "source": [
        "fake_batch = next(iter(gen_imgs_list))\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(fake_batch, padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pytorch fid packet "
      ],
      "metadata": {
        "id": "l5UeQaK8Ikhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPZgg4a2cXjc"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY_okNj4cyuX"
      },
      "outputs": [],
      "source": [
        "!python -m pytorch_fid train/cat-train/cat/ train/cat-train/cat/ --device cpu"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}